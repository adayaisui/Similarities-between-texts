{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de450f0",
   "metadata": {},
   "source": [
    "# Clase Logit\n",
    "Su uso es ser un clasificador logit, i.e. un clasificador de maxima entropia o modelo de regresion logistica\n",
    "\n",
    "\n",
    "### def __init__(self, X, y, alpha=.005)\n",
    "    El constructor que toma 3 argumentos:\n",
    "* X es una matriz [m, n+1], m siendo las observaciones, n siendo el numero de las variables, +1 para agregar un bias term que permite hacer calculos para muestras que tienen valor de 0 (el producto punto de 0's es 0, independientemente de los parametros del modelo)\n",
    "* Y es una matrix [m, 1]\n",
    "* alpha es la tasa de aprendizaje\n",
    "\n",
    "    Ademas, el constructor inicializa los parametros del modelo:\n",
    "* theta, una matriz aleatoria [1, n+1]\n",
    "* loss_hist, donde se guardaran los records de prediccion incorrecta\n",
    "\n",
    "### def add_ordinate(self, X)\n",
    "    Toma una matriz y le agrega una columna de unos. Esto se usa para agregar bias terms a los inputs para simplificar calculos.\n",
    "\n",
    "\n",
    "### def forward(self, X=None)\n",
    "    Toma una matriz opcional X y le aplica la regresion logistica/el modelo logit, calculando el resultado de 1/(1 + e^(-X * theta^T)). Si no se le manda una matriz, usa la matriz creada en el constructor.\n",
    "\n",
    "\n",
    "### def loss(self)\n",
    "    Calcula la entropia cruzada usando el parametro theta. Dicho de otro modo, calcula que tan preciso es el modelo, siendo 0 un modelo perfecto.\n",
    "\n",
    "\n",
    "### def train(self, tol=1e-5, max_iter=10000)\n",
    "    Entrena el modelo usando SGD (Stochastic Gradient Descent). Actualiza el modelo hasta que la loss/perdida sea menor al indice de tolerancia o las iteraciones hayan llegado al maximo; ademas, loss se va guardando en cada iteracion en la variable loss_hist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bca511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: X shape [(1000, 3)]\n",
      "Loading data: y shape [(1000, 1)]\n",
      "params shape: theta [(1, 3)]\n",
      ".\n",
      "loss: 2.904951661065406\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.657592978127068\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.6393161519164824\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.6238638796335727\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.609365798320957\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5957137417114582\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5828503383203255\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5707225055004204\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5592802844081558\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5484768236493319\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5382683389382974\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5286140296380002\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5194759633606881\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5108189388390166\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.5026103353777687\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.4948199554317525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.4874198653220253\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.480384237816117\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.4736891992475402\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "loss: 0.4673126830118281\n",
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................(25704, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (25704,4) and (3,1) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25308/3659970535.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m       \u001b[1;31m# Generate predictions and shape them as grid.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m       \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1x_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m       \u001b[1;31m# Generate plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m       \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25308/3659970535.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_ordinate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m#print(X.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (25704,4) and (3,1) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "    class Logit:\n",
    "    '''\n",
    "    This class is a logit classifier\n",
    "    X in [m, n+1]\n",
    "    n: variables\n",
    "    m: observations\n",
    "    y in [m, 1]\n",
    "    theta in [1, n+1]\n",
    "    np.dot(X, theta.T)\n",
    "\n",
    "    ''' \n",
    "    def __init__(self, X, y, alpha=.005):\n",
    "        self.X = self.add_ordinate(X)\n",
    "        self.y = y\n",
    "        self.theta = np.random.rand(X.shape[1]+1).reshape(1, -1)\n",
    "        self.alpha = alpha # This is the learning rate\n",
    "        self.loss_hist = []\n",
    "        print(f'Loading data: X shape [{self.X.shape}]')\n",
    "        print(f'Loading data: y shape [{self.y.shape}]')\n",
    "        print(f'params shape: theta [{self.theta.shape}]')\n",
    "\n",
    "    def add_ordinate(self, X):\n",
    "        return np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])\n",
    "\n",
    "\n",
    "    def forward(self, X=None):\n",
    "        '''\n",
    "        This function implements:\n",
    "        the logit pass to X. 1/(1 + e-z*theta)\n",
    "        '''\n",
    "        X = self.add_ordinate(X) if not X is None else self.X\n",
    "        #print(X.shape)\n",
    "        return 1/(1 + np.exp(-np.dot(X, self.theta.T)))\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        Computes cross entropy loss\n",
    "        '''\n",
    "        p = self.forward()\n",
    "        return -np.mean(self.y*np.log(p) + (1-self.y)*np.log(1-p))\n",
    "\n",
    "    def train(self, tol=1e-5, max_iter=10000):\n",
    "        iters = 0\n",
    "        loss = np.Inf\n",
    "        #print(iters)\n",
    "        while(loss > tol and iters < max_iter):\n",
    "              print('.', end='', flush=True)\n",
    "              loss = self.loss()\n",
    "              if not iters % 500:\n",
    "                    print(f'\\nloss: {loss}')\n",
    "              p = self.forward().reshape(-1, 1)\n",
    "              self.theta -= -self.alpha*np.mean((self.y - p)*self.X, axis=0)\n",
    "              iters += 1\n",
    "              self.loss_hist.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d269068",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815bc29",
   "metadata": {},
   "source": [
    "    El codigo main genera 2 distribuciones normales multivariadas con medias mean_1 y mean_2 y una matriz de covarianza m_cov. Despues concatena las muestras, y crea una matriz 'y' con etiquetas. Entonces, se crea una instancia de la clase Logit, y se usa el metodo train para entrenar el modelo.\n",
    "    El record de loss, loss_hist, se grafica y guarda en un archivo llamado loss.png.\n",
    "    Finalmente, el codigo grafica el limite de desicion, el cual indica cuales predicciones son correctas y cuales no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    mean_1 = np.array([10, 10]) #matriz de medias de la primer normal multivariada\n",
    "    mean_2 = np.array([7, 7]) # lo mismo para la segunda\n",
    "    m_cov = np.array([[3, 0], [0, 3]]) #matriz de covarianzas con diagonal 3 y lo demás 0\n",
    "    size = 500 #tamaño de la muestra\n",
    "    X = np.concatenate([np.random.multivariate_normal(size=size, mean=mean_1, cov=m_cov), #\n",
    "                          np.random.multivariate_normal(size=size, mean=mean_2, cov=m_cov)],\n",
    "                         axis=0)\n",
    "    y = np.concatenate([np.ones(size).reshape(-1, 1), np.zeros(size).reshape(-1, 1)]) #Una matriz 1000x1 \n",
    "    # Instantiate Model\n",
    "    log1 = Logit(X=X, y=y)\n",
    "    # Forward pass\n",
    "    log1.train()\n",
    "    # Loss\n",
    "    plt.plot(range(len(log1.loss_hist)), log1.loss_hist)\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Plot curves\n",
    "    # ----------------------------------------\n",
    "    (x1_min, x2_min) = X.min(axis=0) - 1\n",
    "    (x1_max, x2_max) = X.max(axis=0) + 1\n",
    "    # Generate axis\n",
    "    x1_axis = np.arange(x1_min, x1_max, step=.1)\n",
    "    x2_axis = np.arange(x2_min, x2_max, step=.1)\n",
    "    # Mesh grid\n",
    "    x1x_, x2x_ = np.meshgrid(x1_axis, x2_axis)\n",
    "    # Shape grid as features\n",
    "    x1x = x1x_.flatten().reshape(-1, 1)\n",
    "    x2x = x2x_.flatten().reshape(-1, 1)\n",
    "    # Add bias term (vector of ones) and transpose (set into 'column' shape\n",
    "    new_X = np.hstack((np.ones(x1x.shape), x1x, x2x))\n",
    "    print(new_X.shape)\n",
    "    # Generate predictions and shape them as grid.\n",
    "    y_hat = log1.forward(new_X).reshape(x1x_.shape)\n",
    "    # Generate plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # Classification contour (notice that we need the grid shape)\n",
    "    ax.contourf(x1x_, x2x_, y_hat)\n",
    "    # Add population points\n",
    "    sns.scatterplot(x='x1', y='x2', hue='class',\n",
    "                  data=pd.DataFrame(np.hstack((y, X)),\n",
    "                                    columns=['class', 'x1', 'x2']),\n",
    "                  ax=ax)\n",
    "    print('Saving classification region plot')\n",
    "    fig.savefig('class_region.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab3d6b",
   "metadata": {},
   "source": [
    "# Muestreo.py\n",
    "\n",
    "    El siguiente codigo preprocesa un archivo JSON que contiene texto, y entrena el modelo de regresion logistica usando una muestra de los datos preprocesados.\n",
    "    \n",
    "    Se usan las siguientes funciones:\n",
    "    1. 'remove_words': Toma un string y le quita todas las palabras contenidas dentro de una lista de palabras pre-definidas en un archivo de texto llamado 'stopwords.txt'\n",
    "    2. 'iterate_list': Toma una lista de strings y una lista de palabras, y le aplica 'remove_words'\n",
    "    3. 'without_spaces': Toma una lista de strings y le quita todos los strings vacios de la lista.\n",
    "    4. 'jaccard_similarity': Toma 2 strings y calcula su distancia Jaccard (la proporcion de palabras compartidas)\n",
    "    5. 'words_in_string': Toma 1 string y una lista de palabras. Regresa 1 si todas las palabras en la lista se encuentran en el string, 0 si no.\n",
    "    \n",
    "    Se carga el archivo JSON en un dataframe de pandas, y se preprocesan los datos usando las funciones anteriormente definidas, ademas de hacer todo en minuscula.\n",
    "    \n",
    "    Entonces, toma una muestra de los datos preprocesados, pasandole los datos como argumentos X y 'y' a una instancia de la clase Logit.\n",
    "    Despues, se grafica el limite de decisiones, que indica que predicciones fueron correctas con matplotlib, y se evalua el modelo calculando su exactitud y precision, e imprime la matriz de confusion, que indica las Falsos Positivos, Falsos Negativos, y Verdaderos Positivos y Negativos.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import logit\n",
    "\n",
    "def remove_words(text):\n",
    "    text_words = text.split()\n",
    "    text_words = [w for w in text_words if w not in words]\n",
    "    text = ' '.join(text_words)\n",
    "    return text\n",
    "\n",
    "def iterate_list(my_list,words):\n",
    "    result = []\n",
    "    for elem in my_list:\n",
    "        if type(elem) == str:\n",
    "            elem = elem.replace(\"?\",\" ?\")\n",
    "            text_words = elem.split()\n",
    "            text_words = [w for w in text_words if w not in words]\n",
    "            text = ' '.join(text_words)\n",
    "            result.append(text)\n",
    "    return result\n",
    "\n",
    "def without_spaces(my_list):\n",
    "    new_list = [x for x in my_list if x != '']\n",
    "    return new_list\n",
    "\n",
    "def jaccard_similarity(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    j = float(len(a.intersection(b))) / len(a.union(b))\n",
    "    return j\n",
    "\n",
    "def words_in_string(words, string):\n",
    "    words_list = words.split()\n",
    "    for word in words_list:\n",
    "        if word not in string:\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "print('\\n--------------------- Preprocessing ---------------------')\n",
    "df = pd.read_json('data.json')\n",
    "\n",
    "print(\"\\n > Todo a lowercase\")\n",
    "\n",
    "df['context'] = df['context'].apply(str.lower)\n",
    "df['questions'] = df['questions'].apply(lambda x: [e.lower() if isinstance(e, str) else e for e in x])\n",
    "df['ans'] = df['ans'].apply(lambda x: [e.lower() if isinstance(e, str) else e for e in x])\n",
    "\n",
    "print(\" > Elimina stopwords\")\n",
    "\n",
    "with open('stopwords.txt', 'r') as f:\n",
    "    words = f.read().split()\n",
    "df['context'] = df['context'].apply(remove_words)\n",
    "df['questions'] = df['questions'].apply(iterate_list,words=words)\n",
    "df['ans'] = df['ans'].apply(iterate_list,words=words)\n",
    "df['context'] = df['context'].str.split('.')\n",
    "\n",
    "print(\" > Elimina strings vacíos\")\n",
    "\n",
    "df['context'] = df['context'].apply(without_spaces)\n",
    "df_exploded = df.explode('context')\n",
    "df_exploded=df_exploded.set_index(['context']).apply(pd.Series.explode).reset_index()\n",
    "\n",
    "print(\" > Calculamos jaccard_similarity\")\n",
    "\n",
    "df_exploded['jaccard_similarity'] = df_exploded.apply(lambda x: jaccard_similarity(x['context'], x['questions']), axis=1)\n",
    "\n",
    "print(\" > Revisa que la oración contenga la respuesta\")\n",
    "\n",
    "df_exploded['contains_ans'] = df_exploded.apply(lambda x: words_in_string(x['ans'], x['context']), axis=1)\n",
    "\n",
    "print(\" > Obtenemos la tabla jaccard\")\n",
    "\n",
    "cols_to_select = [\"jaccard_similarity\", \"contains_ans\"]\n",
    "df_j = df_exploded.loc[:, cols_to_select]\n",
    "\n",
    "print('\\n--------------------- Model training ---------------------')\n",
    "print(\"\\n > Tomamos una muestra de 0.05\")\n",
    "\n",
    "np.random.seed(123454321)\n",
    "sample_size5 = 0.05 # Sample size as a percentage of the total population\n",
    "sample5 = df_j.sample(frac=sample_size5)\n",
    "sample_x5 = sample5.iloc[:,0].values.reshape(-1, 1)\n",
    "sample_y5 = sample5.iloc[:,-1].values.reshape(-1, 1)\n",
    "log5 = logit.Logit(X=sample_x5, y=sample_y5)\n",
    "print(\"\\n > Lo entrenamos\")\n",
    "log5.train()\n",
    "plt.plot(range(len(log5.loss_hist)), log5.loss_hist)\n",
    "plt.savefig('loss_project5.png')\n",
    "\n",
    "print(\"\\n > Tomamos una muestra de 0.01\")\n",
    "\n",
    "np.random.seed(123454321)\n",
    "sample_size1 = 0.01 # Sample size as a percentage of the total population\n",
    "sample1 = df_j.sample(frac=sample_size1)\n",
    "sample_x1 = sample1.iloc[:,0].values.reshape(-1, 1)\n",
    "sample_y1 = sample1.iloc[:,-1].values.reshape(-1, 1)\n",
    "log1 = logit.Logit(X=sample_x1, y=sample_y1)\n",
    "print(\"\\n > Lo entrenamos\")\n",
    "log1.train()\n",
    "plt.plot(range(len(log1.loss_hist)), log1.loss_hist)\n",
    "plt.savefig('loss_project1.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
